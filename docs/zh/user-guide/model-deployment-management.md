# 模型部署管理

你可以在 GPUStack 中进入 `Models - Deployments` 页面管理模型部署。GPUStack 中的模型部署包含一个或多个模型实例的副本。部署时，GPUStack 会根据模型元数据自动计算模型实例的资源需求，并相应地调度到可用的工作节点。

## 部署模型

目前支持来自 [Hugging Face](https://huggingface.co)、[ModelScope](https://modelscope.cn) 以及本地路径的模型。

### 部署 Hugging Face 模型

1. 点击 `Deploy Model` 按钮，在下拉菜单中选择 `Hugging Face`。

2. 在左上角搜索栏按名称搜索 Hugging Face 上的模型，例如 `microsoft/Phi-3-mini-4k-instruct-gguf`。如果你只想搜索 GGUF 模型，勾选 “GGUF” 复选框。

3. 在 `Available Files` 中选择所需量化格式的文件。

4. 按需调整 `Name` 和 `Replicas`。

5. 如需高级配置，展开 `Advanced`。详情参见[高级模型配置](#advanced-model-configuration)一节。

6. 点击 `Save` 按钮。

### 部署 ModelScope 模型

1. 点击 `Deploy Model` 按钮，在下拉菜单中选择 `ModelScope`。

2. 在左上角搜索栏按名称搜索 ModelScope 上的模型，例如 `Qwen/Qwen2-0.5B-Instruct`。如果你只想搜索 GGUF 模型，勾选 “GGUF” 复选框。

3. 在 `Available Files` 中选择所需量化格式的文件。

4. 按需调整 `Name` 和 `Replicas`。

5. 如需高级配置，展开 `Advanced`。详情参见[高级模型配置](#advanced-model-configuration)一节。

6. 点击 `Save` 按钮。

### 部署本地路径模型

你可以从本地路径部署模型。模型路径可以是工作节点上的目录（例如已下载的 Hugging Face 模型目录），也可以是文件（例如 GGUF 模型文件）。这在内网隔离（air-gapped）环境中很有用。

!!!note

    1. GPUStack 不会在调度时检查模型路径的有效性，如果模型路径不可访问，可能导致部署失败。建议确保模型路径在所有工作节点上均可访问（例如使用 NFS、rsync 等）。你也可以使用 Worker 选择器配置将模型部署到特定的工作节点。
    2. 除非服务器能够访问相同的模型路径，否则 GPUStack 无法评估模型的资源需求。因此，你可能会看到已部署模型的显存/内存分配为空。为此，建议在服务器上将模型文件提供在相同路径下。或者，你也可以自定义后端参数（例如 `tensor-split`）来配置模型如何在多块 GPU 上分布。

要部署本地路径模型：

1. 点击 `Deploy Model` 按钮，在下拉菜单中选择 `Local Path`。

2. 填写部署的 `Name`。

3. 填写 `Model Path`。

4. 按需调整 `Replicas`。

5. 如需高级配置，展开 `Advanced`。详情参见[高级模型配置](#advanced-model-configuration)一节。

6. 点击 `Save` 按钮。

## 编辑模型部署

1. 在部署列表页找到你想编辑的模型部署。
2. 点击 `Operations` 列中的 `Edit` 按钮。
3. 按需更新属性。例如，调整 `Replicas` 以扩缩容。
4. 点击 `Save` 按钮。

!!! note

    编辑模型部署后，配置不会应用到已有的模型实例。你需要删除现有的模型实例。GPUStack 将基于更新后的模型配置重新创建新实例。

## 停止模型部署

停止模型部署将删除所有模型实例并释放资源。这等同于将模型缩容到 0 个副本。

1. 在部署列表页找到你想停止的模型部署。
2. 点击 `Operations` 列中的省略号按钮，然后选择 `Stop`。
3. 确认操作。

## 启动模型部署

启动模型部署等同于将模型扩容到 1 个副本。

1. 在部署列表页找到你想启动的模型部署。
2. 点击 `Operations` 列中的省略号按钮，然后选择 `Start`。

## 删除模型部署

1. 在部署列表页找到你想删除的模型部署。
2. 点击 `Operations` 列中的省略号按钮，然后选择 `Delete`。
3. 确认删除。

## 查看模型实例

1. 在部署列表页找到你想查看的模型部署。
2. 点击 `>` 符号以查看该部署的实例列表。

## 删除模型实例

1. 在部署列表页找到你想查看的模型部署。
2. 点击 `>` 符号以查看该部署的实例列表。
3. 找到你想删除的模型实例。
4. 点击该实例在 `Operations` 列中的省略号按钮，然后选择 `Delete`。
5. 确认删除。

!!! note

    删除某个模型实例后，如有必要，GPUStack 会重新创建新实例以满足部署所期望的副本数。

## 查看模型实例日志

1. 在部署列表页找到你想查看的模型部署。
2. 点击 `>` 符号以查看该部署的实例列表。
3. 找到你想查看的模型实例。
4. 点击该实例在 `Operations` 列中的 `View Logs` 按钮。

<a id="advanced-model-configuration"></a>

## 高级模型配置

GPUStack 支持为模型部署进行定制化配置。

### 模型类别

模型类别有助于组织和筛选模型。默认情况下，GPUStack 会基于模型元数据自动检测模型类别。你也可以在下拉列表中手动选择类别。

### 调度类型

#### 自动

GPUStack 会根据当前资源可用性自动将模型实例调度到合适的 GPU/工作节点。

- 放置策略

  - Spread：使整个集群的资源在所有工作节点上相对均匀分布。这可能会在单个工作节点上产生更多的资源碎片。

  - Binpack：优先提高集群整体资源利用率，减少工作节点/GPU 上的资源碎片。

- Worker 选择器

  配置后，调度器会将模型实例部署到包含指定标签的工作节点上。

  1. 进入 `Workers` 页面并编辑目标工作节点。在标签区域为该节点添加自定义标签。

  2. 前往 `Deployments` 页面并点击 `Deploy Model`。展开 `Advanced`，在 `Worker Selector` 配置中输入先前分配的工作节点标签。部署时，模型实例将根据这些标签分配到相应的工作节点。

#### 手动

该调度类型允许用户指定将模型实例部署到哪块 GPU 上。

- GPU 选择器

  从列表中选择一块或多块 GPU。在资源允许的情况下，模型实例将尝试部署到所选 GPU 上。

### 后端

推理后端。目前，GPUStack 支持三种后端：llama-box、vLLM 和 vox-box。GPUStack 会根据模型的配置自动选择后端。

更多详情请参见[推理后端](inference-backends.md)部分。

### 后端版本

指定后端版本，例如 `v1.0.0`。版本格式与可用性取决于所选后端。该选项有助于确保兼容性，或使用特定版本引入的功能。请参阅[固定后端版本](pinned-backend-versions.md)了解更多信息。

### 后端参数

输入在运行模型时要自定义的后端参数。参数格式应为 `--parameter=value`、`--bool-parameter`，或将 `--parameter` 与 `value` 分开填写。
例如，对 llama-box 使用 `--ctx-size=8192`。

支持的参数完整列表请参见[推理后端](inference-backends.md)部分。

### 环境变量

运行模型时使用的环境变量。这些变量会在启动时传递给后端进程。

### 允许 CPU 卸载

!!! note

    仅适用于 llama-box 后端。

启用 CPU 卸载后，GPUStack 会优先将尽可能多的层加载到 GPU 以优化性能。若 GPU 资源不足，部分层将卸载到 CPU；只有在没有可用 GPU 时才会完全使用 CPU 进行推理。

### 允许跨工作节点进行分布式推理

!!! note

    适用于 llama-box、vLLM 和 MindIE 后端。

启用跨多个工作节点的分布式推理。主模型实例会与一个或多个其他工作节点上的后端实例通信，将计算任务卸载给它们。

### 发生错误时自动重启

启用当模型实例遇到错误时的自动重启功能。该功能可确保模型实例的高可用性与可靠性。如果发生错误，GPUStack 会使用指数退避策略自动尝试重启模型实例。重启尝试之间的延迟将按指数增长，最长不超过 5 分钟。此方式可避免在持续错误情况下因频繁重启而使系统不堪重负。