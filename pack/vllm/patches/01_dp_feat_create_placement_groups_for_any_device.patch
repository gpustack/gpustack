diff --git a/vllm/v1/engine/utils.py b/vllm/v1/engine/utils.py
index c40124194..51590beef 100644
--- a/vllm/v1/engine/utils.py
+++ b/vllm/v1/engine/utils.py
@@ -244,7 +244,7 @@ class CoreEngineActorManager:
         local_engine_count = \
             vllm_config.parallel_config.data_parallel_size_local
 
-        nodes = sorted(list_nodes(),
+        nodes = sorted(list_nodes(filters=[("state", "=", "ALIVE")]),
                        key=lambda node: node.node_ip != dp_master_ip)
         assert nodes[0].node_ip == dp_master_ip, (
             "The first node must be the head node")
@@ -256,20 +256,26 @@ class CoreEngineActorManager:
         placement_groups: list[PlacementGroup] = []
         local_dp_ranks: list[int] = []
 
+        from vllm.platforms import current_platform
+
+        device_key = "GPU"
+        if current_platform.ray_device_key:
+            device_key = current_platform.ray_device_key
+
         for node in nodes:
             node_ip = node.node_ip
             node_resources = available_resources[node.node_id]
             # For now, each DP rank can only be assigned to one node
             # TODO(rui): support allocating a single DP rank
             # to multiple nodes
-            available_engine_count = int(node_resources["GPU"]) // world_size
+            available_engine_count = int(node_resources[device_key]) // world_size
             if node_ip == dp_master_ip:
                 assert available_engine_count >= local_engine_count, (
                     "Not enough resources to allocate DP ranks "
                     f"on DP master node {node_ip}")
                 for i in range(local_engine_count):
                     bundles = [{
-                        "GPU": 1.0,
+                        device_key: 1.0,
                         "node:" + dp_master_ip: 0.001
                     }] * world_size + [{
                         "CPU": 1.0
@@ -285,7 +291,7 @@ class CoreEngineActorManager:
                 for i in range(available_engine_count):
                     if len(placement_groups) == dp_size:
                         break
-                    bundles = [{"GPU": 1.0}] * world_size + [{"CPU": 1.0}]
+                    bundles = [{device_key: 1.0}] * world_size + [{"CPU": 1.0}]
                     pg = ray.util.placement_group(
                         name=f"dp_rank_{len(placement_groups)}",
                         strategy="STRICT_PACK",
