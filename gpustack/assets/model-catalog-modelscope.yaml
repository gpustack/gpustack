draft_models:
- name: Qwen3-8B-EAGLE3
  algorithm: eagle3
  source: model_scope
  model_scope_model_id: gpustack/qwen3_8b_eagle3
- name: Qwen3-30B-A3B-EAGLE3
  algorithm: eagle3
  source: model_scope
  model_scope_model_id: gpustack/qwen3_30b_moe_eagle3
- name: Qwen3-235B-A22B-EAGLE3
  algorithm: eagle3
  source: model_scope
  model_scope_model_id: gpustack/Qwen3-235B-A22B-EAGLE3
- name: gpt-oss-120b-EAGLE3
  algorithm: eagle3
  source: model_scope
  model_scope_model_id: gpustack/EAGLE3-gpt-oss-120b-bf16
model_sets:
- name: Qwen3 0.6B
  description: Qwen3 is a family of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support.
  home: https://qwenlm.github.io
  icon: /static/catalog_icons/qwen.png
  size: 0.6
  categories:
    - llm
  capabilities:
    - context/128K
    - tools
  licenses:
    - apache-2.0
  release_date: "2025-04-19"
  specs:
    # Ascend NPUs
    - mode: throughput
      quantization: BF16
      gpu_filters:
        vendor: ascend
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-0.6B
      backend: MindIE
      backend_parameters:
        - --max-seq-len=8192
    # Other GPUs
    - mode: standard
      quantization: BF16
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-0.6B
      backend: vLLM
      backend_parameters:
        - --reasoning-parser=deepseek_r1
        - --max-model-len=8192
- name: Qwen3 8B
  description: Qwen3 is a family of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support.
  home: https://qwenlm.github.io
  icon: /static/catalog_icons/qwen.png
  size: 8
  categories:
    - llm
  capabilities:
    - context/128K
    - tools
  licenses:
    - apache-2.0
  release_date: "2025-04-19"
  specs:
    # Ascend NPUs
    - mode: throughput
      quantization: W8A8
      gpu_filters:
        vendor: ascend
        vendor_variant: "910b"
      source: model_scope
      model_scope_model_id: vllm-ascend/Qwen3-8B-W8A8
      backend: MindIE
      backend_parameters:
        - --enable-prefix-caching
        - --max-seq-len=32768
    - mode: standard
      quantization: BF16
      gpu_filters:
        vendor: ascend
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-8B
      backend: MindIE
      backend_parameters:
        - --max-seq-len=32768
    # Other GPUs
    - mode: throughput
      quantization: FP8
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-8B-FP8
      backend: vLLM
      backend_parameters:
        - --reasoning-parser=deepseek_r1
        - --max-model-len=32768
    - mode: standard
      quantization: BF16
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-8B
      backend: vLLM
      backend_parameters:
        - --reasoning-parser=deepseek_r1
        - --max-model-len=32768
- name: Qwen3 14B
  description: Qwen3 is a family of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support.
  home: https://qwenlm.github.io
  icon: /static/catalog_icons/qwen.png
  size: 14
  categories:
    - llm
  capabilities:
    - context/128K
    - tools
  licenses:
    - apache-2.0
  release_date: "2025-04-19"
  specs:
    # Ascend NPUs
    - mode: throughput
      quantization: BF16
      gpu_filters:
        vendor: ascend
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-14B
      backend: MindIE
      backend_parameters:
        - --max-seq-len=32768
    # Other GPUs
    - mode: throughput
      quantization: FP8
      gpu_filters:
        vendor: nvidia
        compute_capability: ">=9.0" # Hopper or later
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-14B-FP8
      backend: SGLang
      backend_parameters:
        - --reasoning-parser=qwen3
        - --context-length=32768
    - mode: throughput
      quantization: FP8
      gpu_filters:
        vendor: nvidia
        compute_capability: "<9.0" # Before Hopper
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-14B-FP8
      backend: vLLM
      backend_parameters:
        - --reasoning-parser=deepseek_r1
        - --max-model-len=32768
    - mode: standard
      quantization: BF16
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-14B
      backend: vLLM
      backend_parameters:
        - --reasoning-parser=deepseek_r1
        - --max-model-len=32768
- name: Qwen3 32B
  description: Qwen3 is a family of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support.
  home: https://qwenlm.github.io
  icon: /static/catalog_icons/qwen.png
  size: 32
  categories:
    - llm
  capabilities:
    - context/128K
    - tools
  licenses:
    - apache-2.0
  release_date: "2025-04-19"
  specs:
    # Ascend NPUs
    - mode: throughput
      quantization: W8A8
      gpu_filters:
        vendor: ascend
        vendor_variant: "910b"
      source: model_scope
      model_scope_model_id: vllm-ascend/Qwen3-32B-W8A8
      backend: MindIE
      backend_parameters:
        - --enable-prefix-caching
        - --max-seq-len=32768
    - mode: standard
      quantization: BF16
      gpu_filters:
        vendor: ascend
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-32B
      backend: MindIE
      backend_parameters:
        - --max-seq-len=32768
    # Other GPUs
    - mode: throughput
      quantization: FP8
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-32B-FP8
      backend: vLLM
      backend_parameters:
        - --reasoning-parser=deepseek_r1
        - --max-model-len=32768
    - mode: standard
      quantization: BF16
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-32B
      backend: vLLM
      backend_parameters:
        - --reasoning-parser=deepseek_r1
        - --max-model-len=32768
- name: Qwen3 30B A3B Instruct 2507
  description: Qwen3 is a family of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support.
  home: https://qwenlm.github.io
  icon: /static/catalog_icons/qwen.png
  size: 30
  activated_size: 3
  categories:
    - llm
  capabilities:
    - context/256K
    - tools
  licenses:
    - apache-2.0
  release_date: "2025-07-21"
  specs:
    # Ascend NPUs
    - mode: throughput
      quantization: BF16
      gpu_filters:
        vendor: ascend
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-30B-A3B-Instruct-2507
      backend: MindIE
      backend_parameters:
        - --max-seq-len=32768
    # Other GPUs
    - mode: throughput
      quantization: FP8
      gpu_filters:
        vendor: nvidia
        compute_capability: ">=9.0" # Hopper or later
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-30B-A3B-Instruct-2507-FP8
      backend: SGLang
      backend_parameters:
        - --tool-call-parser=qwen25
        - --context-length=32768
    - mode: throughput
      quantization: FP8
      gpu_filters:
        vendor: nvidia
        compute_capability: "<9.0" # Before Hopper
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-30B-A3B-Instruct-2507-FP8
      backend: vLLM
      backend_parameters:
        - --tool-call-parser=hermes
        - --enable-auto-tool-choice
        - --max-model-len=32768
    - mode: standard
      quantization: BF16
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-30B-A3B-Instruct-2507
      backend: vLLM
      backend_parameters:
        - --tool-call-parser=hermes
        - --enable-auto-tool-choice
        - --max-model-len=32768
- name: Qwen3 30B A3B Thinking 2507
  description: Qwen3 is a family of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support.
  home: https://qwenlm.github.io
  icon: /static/catalog_icons/qwen.png
  size: 30
  activated_size: 3 
  categories:
    - llm
  capabilities:
    - context/256K
    - tools
  licenses:
    - apache-2.0
  release_date: "2025-07-21"
  specs:
    # Ascend NPUs
    - mode: throughput
      quantization: BF16
      gpu_filters:
        vendor: ascend
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-30B-A3B-Thinking-2507
      backend: MindIE
      backend_parameters:
        - --max-seq-len=32768
    # Other GPUs
    - mode: throughput
      quantization: FP8
      gpu_filters:
        vendor: nvidia
        compute_capability: ">=9.0" # Hopper or later
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-30B-A3B-Thinking-2507-FP8
      backend: SGLang
      backend_parameters:
        - --reasoning-parser=deepseek-r1
        - --tool-call-parser=qwen25
        - --context-length=32768
    - mode: throughput
      quantization: FP8
      gpu_filters:
        vendor: nvidia
        compute_capability: "<9.0" # Before Hopper
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-30B-A3B-Thinking-2507-FP8
      backend: vLLM
      backend_parameters:
        - --reasoning-parser=deepseek_r1
        - --tool-call-parser=hermes
        - --enable-auto-tool-choice
        - --max-model-len=32768
    - mode: standard
      quantization: BF16
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-30B-A3B-Thinking-2507
      backend: vLLM
      backend_parameters:
        - --reasoning-parser=deepseek_r1
        - --tool-call-parser=hermes
        - --enable-auto-tool-choice
        - --max-model-len=32768
- name: Qwen3 235B A22B Instruct 2507
  description: The updated version of the Qwen3-235B-A22B non-thinking mode.
  home: https://qwenlm.github.io
  icon: /static/catalog_icons/qwen.png
  size: 235
  activated_size: 22
  categories:
    - llm
  capabilities:
    - context/1M
    - tools
  licenses:
    - apache-2.0
  release_date: "2025-07-21"
  specs:
    # Ascend NPUs
    - mode: throughput
      quantization: BF16
      gpu_filters:
        vendor: ascend
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-235B-A22B-Instruct-2507
      backend: MindIE
      backend_parameters:
        - --max-seq-len=65536
    # Other GPUs
    - mode: throughput
      quantization: FP8
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-235B-A22B-Instruct-2507-FP8
      backend: vLLM
      backend_parameters:
        - --tool-call-parser=hermes
        - --enable-auto-tool-choice
        - --max-model-len=65536
    - mode: standard
      quantization: BF16
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-235B-A22B-Instruct-2507
      backend: vLLM
      backend_parameters:
        - --tool-call-parser=hermes
        - --enable-auto-tool-choice
        - --max-model-len=65536
- name: Qwen3 235B A22B Thinking 2507
  description: The updated version of the Qwen3-235B-A22B thinking mode.
  home: https://qwenlm.github.io
  icon: /static/catalog_icons/qwen.png
  size: 235
  activated_size: 22
  categories:
    - llm
  capabilities:
    - context/1M
    - tools
  licenses:
    - apache-2.0
  release_date: "2025-07-21"
  specs:
    # Ascend NPUs
    - mode: throughput
      quantization: BF16
      gpu_filters:
        vendor: ascend
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-235B-A22B-Thinking-2507
      backend: MindIE
      backend_parameters:
        - --max-seq-len=65536
    # Other GPUs
    - mode: throughput
      quantization: FP8
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-235B-A22B-Thinking-2507-FP8
      backend: vLLM
      backend_parameters:
        - --reasoning-parser=deepseek_r1
        - --tool-call-parser=hermes
        - --enable-auto-tool-choice
        - --max-model-len=65536
    - mode: standard
      quantization: BF16
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-235B-A22B-Thinking-2507
      backend: vLLM
      backend_parameters:
        - --reasoning-parser=deepseek_r1
        - --tool-call-parser=hermes
        - --enable-auto-tool-choice
        - --max-model-len=65536
- name: GLM-4.7
  description: GLM-4.7 is a large language model developed by Zhipu AI, featuring advanced agentic, reasoning, and coding capabilities.
  home: https://z.ai
  icon: /static/catalog_icons/zai.png
  size: 355
  activated_size: 32
  categories:
    - llm
  capabilities:
    - context/1M
    - reasoning
    - tools
  licenses:
    - mit
  release_date: "2025-12-22"
  specs:
    # TODO: tool-call-parser glm47 not yet available in the latest vLLM/SGLang release
    - mode: throughput
      quantization: FP8
      gpu_filters:
        vendor: nvidia
        compute_capability: ">=9.0" # Hopper or later
      source: model_scope
      model_scope_model_id: ZhipuAI/GLM-4.7-FP8
      backend: SGLang
      backend_parameters:
        - --reasoning-parser=glm45
        - --context-length=65536
    - mode: throughput
      quantization: FP8
      gpu_filters:
        vendor: nvidia
        compute_capability: "<9.0" # Before Hopper
      source: model_scope
      model_scope_model_id: ZhipuAI/GLM-4.7-FP8
      backend: vLLM
      backend_parameters:
        - --reasoning-parser=glm45
        - --max-model-len=65536
    - mode: standard
      quantization: BF16
      source: model_scope
      model_scope_model_id: ZhipuAI/GLM-4.7
      backend: vLLM
      backend_parameters:
        - --reasoning-parser=glm45
        - --max-model-len=65536
- name: GLM-4.6
  description: GLM-4.6 is a large language model developed by Zhipu AI, featuring advanced agentic, reasoning, and coding capabilities.
  home: https://z.ai
  icon: /static/catalog_icons/zai.png
  size: 355
  activated_size: 32
  categories:
    - llm
  capabilities:
    - context/1M
    - reasoning
    - tools
  licenses:
    - mit
  release_date: "2025-09-30"
  specs:
    - mode: throughput
      quantization: FP8
      gpu_filters:
        vendor: nvidia
        compute_capability: ">=9.0" # Hopper or later
      source: model_scope
      model_scope_model_id: ZhipuAI/GLM-4.6-FP8
      backend: SGLang
      backend_parameters:
        - --tool-call-parser=glm
        - --reasoning-parser=glm45
        - --context-length=65536
    - mode: throughput
      quantization: FP8
      gpu_filters:
        vendor: nvidia
        compute_capability: "<9.0" # Before Hopper
      source: model_scope
      model_scope_model_id: ZhipuAI/GLM-4.6-FP8
      backend: vLLM
      backend_parameters:
        - --reasoning-parser=glm45
        - --tool-call-parser=glm45
        - --enable-auto-tool-choice
        - --max-model-len=65536
    - mode: standard
      quantization: BF16
      source: model_scope
      model_scope_model_id: ZhipuAI/GLM-4.6
      backend: vLLM
      backend_parameters:
        - --reasoning-parser=glm45
        - --tool-call-parser=glm45
        - --enable-auto-tool-choice
        - --max-model-len=65536
- name: gpt-oss 120B
  description: The gpt-oss series is OpenAI's family of open-weight models, designed for powerful reasoning, agentic tasks, and versatile developer use cases.
  home: https://openai.com
  icon: /static/catalog_icons/openai.png
  categories:
    - llm
  capabilities:
    - context/128K
  size: 120
  licenses:
    - apache-2.0
  release_date: "2025-08-05"
  specs:
    - mode: throughput
      quantization: "MXFP4"
      source: model_scope
      model_scope_model_id: openai-mirror/gpt-oss-120b
      backend: vLLM
      backend_parameters:
        - --max-model-len=32768
        - --tool-call-parser=openai
        - --enable-auto-tool-choice
        - --async-scheduling
    - mode: standard
      quantization: "MXFP4"
      source: model_scope
      model_scope_model_id: openai-mirror/gpt-oss-120b
      backend: vLLM
      backend_parameters:
        - --max-model-len=32768
        - --tool-call-parser=openai
        - --enable-auto-tool-choice
- name: gpt-oss 20B
  description: The gpt-oss series is OpenAI's family of open-weight models, designed for powerful reasoning, agentic tasks, and versatile developer use cases.
  home: https://openai.com
  icon: /static/catalog_icons/openai.png
  categories:
    - llm
  capabilities:
    - context/128K
  size: 20
  licenses:
    - apache-2.0
  release_date: "2025-08-05"
  specs:
    - mode: throughput
      quantization: "MXFP4"
      source: model_scope
      model_scope_model_id: openai-mirror/gpt-oss-20b
      backend: vLLM
      backend_parameters:
        - --max-model-len=32768
        - --tool-call-parser=openai
        - --enable-auto-tool-choice
        - --async-scheduling
    - mode: standard
      quantization: "MXFP4"
      source: model_scope
      model_scope_model_id: openai-mirror/gpt-oss-20b
      backend: vLLM
      backend_parameters:
        - --max-model-len=32768
        - --tool-call-parser=openai
        - --enable-auto-tool-choice
- name: Deepseek R1 0528
  description: DeepSeek-R1-0528 is a minor version of the DeepSeek R1 model that features enhanced reasoning depth and inference capabilities. These improvements are achieved through increased computational resources and algorithmic optimizations applied during post-training. The model delivers strong performance across a range of benchmark evaluations, including mathematics, programming, and general logic, with overall capabilities approaching those of leading models such as O3 and Gemini 2.5 Pro.
  home: https://www.deepseek.com
  icon: /static/catalog_icons/deepseek.png
  categories:
    - llm
  capabilities:
    - context/128K
  size: 671
  licenses:
    - mit
  release_date: "2025-05-28"
  specs:
    - mode: throughput
      quantization: FP8
      gpu_filters:
        vendor: nvidia
        compute_capability: ">=9.0" # Hopper or later
      source: model_scope
      model_scope_model_id: deepseek-ai/DeepSeek-R1-0528
      backend: SGLang
      backend_parameters:
        - --enable-dp-attention
        - --context-length=32768
    - mode: standard
      quantization: FP8
      source: model_scope
      model_scope_model_id: deepseek-ai/DeepSeek-R1-0528
      backend: vLLM
      backend_parameters:
        - --max-model-len=32768
- name: DeepSeek-OCR
  description: DeepSeek-OCR is an advanced optical character recognition (OCR) model developed by DeepSeek AI. It is designed to accurately extract text from images and scanned documents.
  home: https://www.deepseek.com
  icon: /static/catalog_icons/deepseek.png
  size: 3
  categories:
    - llm
  licenses:
    - mit
  release_date: "2025-10-20"
  specs:
    - mode: standard
      quantization: "BF16"
      gpu_filters:
        vendor: 
          - nvidia
          - amd
      source: model_scope
      model_scope_model_id: deepseek-ai/DeepSeek-OCR
      backend: vLLM
      backend_version: 0.11.2
      backend_parameters:
        - --logits_processors=vllm.model_executor.models.deepseek_ocr:NGramPerReqLogitsProcessor
        - --no-enable-prefix-caching
        - --mm-processor-cache-gb=0
- name: Deepseek V3.2
  description: 'DeepSeek-V3.2 is a model that balances computational efficiency with strong reasoning and agent capabilities through three technical innovations: DeepSeek Sparse Attention (DSA), Scalable Reinforcement Learning Framework, Large-Scale Agentic Task Synthesis Pipeline.'
  home: https://www.deepseek.com
  icon: /static/catalog_icons/deepseek.png
  categories:
    - llm
  capabilities:
    - context/128K
  size: 685
  licenses:
    - mit
  release_date: "2025-12-01"
  specs:
    - mode: throughput
      quantization: FP8
      gpu_filters:
        vendor: nvidia
        compute_capability: ">=9.0" # Hopper or later
      source: model_scope
      model_scope_model_id: deepseek-ai/DeepSeek-V3.2
      backend: SGLang
      backend_version: 0.5.6.post2
      backend_parameters:
        - --enable-dp-attention
        - --context-length=32768
    - mode: standard
      quantization: FP8
      source: model_scope
      model_scope_model_id: deepseek-ai/DeepSeek-V3.2
      backend: vLLM
      backend_version: 0.13.0
      backend_parameters:
        - --max-model-len=32768
        - --tokenizer-mode=deepseek_v32
        - --reasoning-parser=deepseek_v3
- name: Deepseek V3.2 Speciale
  description: This model is the high-compute variant of DeepSeek-V3.2, surpasses GPT-5 and matches Gemini-3.0-Pro in reasoning, achieving gold-medal level performance in the 2025 IMO and IOI competitions. 
  home: https://www.deepseek.com
  icon: /static/catalog_icons/deepseek.png
  categories:
    - llm
  capabilities:
    - context/128K
  size: 685
  licenses:
    - mit
  release_date: "2025-12-01"
  specs:
    - mode: throughput
      quantization: FP8
      gpu_filters:
        vendor: nvidia
        compute_capability: ">=9.0" # Hopper or later
      source: model_scope
      model_scope_model_id: deepseek-ai/DeepSeek-V3.2-Speciale
      backend: SGLang
      backend_version: 0.5.6.post2
      backend_parameters:
        - --enable-dp-attention
        - --context-length=32768
    - mode: standard
      quantization: FP8
      source: model_scope
      model_scope_model_id: deepseek-ai/DeepSeek-V3.2-Speciale
      backend: vLLM
      backend_version: 0.13.0
      backend_parameters:
        - --max-model-len=32768
        - --tokenizer-mode=deepseek_v32
        - --reasoning-parser=deepseek_v3
# Embedding models
- name: Qwen3 Embedding 0.6B
  description: Qwen3-Embedding is a multilingual embedding model series optimized for retrieval, clustering, classification, and bitext mining. It supports 100+ languages, with flexible vector dimensions and instruction tuning.
  home: https://qwenlm.github.io
  icon: /static/catalog_icons/qwen.png
  size: 0.6
  categories:
    - embedding
  capabilities:
    - dimensions/4096
    - max_tokens/32K
  licenses:
    - apache-2.0
  release_date: "2025-06-09"
  specs:
    - mode: standard
      quantization: "BF16"
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-Embedding-0.6B
      categories:
        - embedding
      backend: vLLM
- name: Qwen3 Embedding 4B
  description: Qwen3-Embedding is a multilingual embedding model series optimized for retrieval, clustering, classification, and bitext mining. It supports 100+ languages, with flexible vector dimensions and instruction tuning.
  home: https://qwenlm.github.io
  icon: /static/catalog_icons/qwen.png
  size: 4
  categories:
    - embedding
  capabilities:
    - dimensions/4096
    - max_tokens/32K
  licenses:
    - apache-2.0
  release_date: "2025-06-09"
  specs:
    - mode: standard
      quantization: "BF16"
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-Embedding-4B
      categories:
        - embedding
      backend: vLLM
- name: Qwen3 Embedding 8B
  description: Qwen3-Embedding is a multilingual embedding model series optimized for retrieval, clustering, classification, and bitext mining. It supports 100+ languages, with flexible vector dimensions and instruction tuning.
  home: https://qwenlm.github.io
  icon: /static/catalog_icons/qwen.png
  size: 8
  categories:
    - embedding
  capabilities:
    - dimensions/4096
    - max_tokens/32K
  licenses:
    - apache-2.0
  release_date: "2025-06-09"
  specs:
    - mode: standard
      quantization: "BF16"
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-Embedding-8B
      categories:
        - embedding
      backend: vLLM
# Reranker models
- name: Qwen3 Reranker 0.6B
  description: Qwen3-Reranker is a multilingual text reranking model series optimized for retrieval, clustering, classification, and bitext mining. It supports 100+ languages, with flexible vector dimensions and instruction tuning.
  home: https://qwenlm.github.io
  icon: /static/catalog_icons/qwen.png
  size: 0.6
  categories:
    - reranker
  capabilities:
    - max_tokens/32K
  licenses:
    - apache-2.0
  release_date: "2025-06-09"
  specs:
    - mode: standard
      quantization: BF16
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-Reranker-0.6B
      categories:
        - reranker
      backend: vLLM
      backend_parameters:
        - '--hf_overrides={"architectures":["Qwen3ForSequenceClassification"],"classifier_from_token":["no","yes"],"is_original_qwen3_reranker":true}'
- name: Qwen3 Reranker 4B
  description: Qwen3-Reranker is a multilingual text reranking model series optimized for retrieval, clustering, classification, and bitext mining. It supports 100+ languages, with flexible vector dimensions and instruction tuning.
  home: https://qwenlm.github.io
  icon: /static/catalog_icons/qwen.png
  size: 4
  categories:
    - reranker
  capabilities:
    - max_tokens/32K
  licenses:
    - apache-2.0
  release_date: "2025-06-09"
  specs:
    - mode: standard
      quantization: BF16
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-Reranker-4B
      categories:
        - reranker
      backend: vLLM
      backend_parameters:
        - '--hf_overrides={"architectures":["Qwen3ForSequenceClassification"],"classifier_from_token":["no","yes"],"is_original_qwen3_reranker":true}'
- name: Qwen3 Reranker 8B
  description: Qwen3-Reranker is a multilingual text reranking model series optimized for retrieval, clustering, classification, and bitext mining. It supports 100+ languages, with flexible vector dimensions and instruction tuning.
  home: https://qwenlm.github.io
  icon: /static/catalog_icons/qwen.png
  size: 8
  categories:
    - reranker
  capabilities:
    - max_tokens/32K
  licenses:
    - apache-2.0
  release_date: "2025-06-09"
  specs:
    - mode: standard
      quantization: BF16
      source: model_scope
      model_scope_model_id: Qwen/Qwen3-Reranker-8B
      categories:
        - reranker
      backend: vLLM
      backend_parameters:
        - '--hf_overrides={"architectures":["Qwen3ForSequenceClassification"],"classifier_from_token":["no","yes"],"is_original_qwen3_reranker":true}'
# Image models
- name: FLUX.1 Dev
  description: FLUX.1 [dev] is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions.
  home: https://blackforestlabs.ai
  icon: /static/catalog_icons/blackforestlabs.png
  size: 12
  categories:
    - image
  licenses:
    - flux-1-dev-non-commercial-license
  release_date: "2024-08-02"
  specs:
    - mode: standard
      quantization: "BF16"
      gpu_filters:
        vendor: nvidia
      source: model_scope
      model_scope_model_id: black-forest-labs/FLUX.1-dev
      backend: SGLang
      backend_version: 0.5.6.post2
      env:
        GPUSTACK_MODEL_VRAM_CLAIM: "37580963840" # 35 GiB, observed empirically
- name: Qwen-Image
  description: Qwen-Image is an image generation foundation model in the Qwen series that achieves significant advances in complex text rendering and precise image editing.
  home: https://qwen.ai
  icon: /static/catalog_icons/qwen.png
  size: 20
  categories:
    - image
  licenses:
    - apache-2.0
  release_date: "2025-08-04"
  specs:
    - mode: standard
      quantization: "BF16"
      gpu_filters:
        vendor: nvidia
      source: model_scope
      model_scope_model_id: Qwen/Qwen-Image
      backend: SGLang
      backend_version: 0.5.6.post2
      env:
        # Miss the weight files in sub-directories at the moment, claim empirical value here.
        GPUSTACK_MODEL_VRAM_CLAIM: "53687091200" # 50 GiB
- name: Qwen-Image-Edit
  description: Built upon our 20B Qwen-Image model, Qwen-Image-Edit successfully extends Qwen-Imageâ€™s unique text rendering capabilities to image editing tasks, enabling precise text editing.
  home: https://qwen.ai
  icon: /static/catalog_icons/qwen.png
  size: 20
  categories:
    - image
  licenses:
    - apache-2.0
  release_date: "2025-08-19"
  specs:
    - mode: standard
      quantization: "BF16"
      gpu_filters:
        vendor: nvidia
      source: model_scope
      model_scope_model_id: Qwen/Qwen-Image-Edit
      backend: SGLang
      backend_version: 0.5.6.post2
      env:
        GPUSTACK_MODEL_VRAM_CLAIM: "64424509440" # 60 GiB, , observed empirically
- name: Z-Image-Turbo
  description: Z-Image is a powerful and highly efficient image generation model with 6B parameters.
  home: https://qwen.ai
  icon: /static/catalog_icons/qwen.png
  size: 6
  categories:
    - image
  licenses:
    - apache-2.0
  release_date: "2025-11-27"
  specs:
    - mode: standard
      quantization: "BF16"
      gpu_filters:
        vendor: nvidia
      source: model_scope
      model_scope_model_id: Tongyi-MAI/Z-Image-Turbo
      backend: SGLang
      backend_version: 0.5.6.post2
      env:
        GPUSTACK_MODEL_VRAM_CLAIM: "35433480192" # 33 GiB, observed empirically
